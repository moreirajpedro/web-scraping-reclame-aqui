{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro precisamos instalar o Selenium, uma ferramenta open-source para interagir de forma automatica com browsers web\n",
    "!pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos utilizar o Google Chrome nesse scrapping, mas caso deseje utilizar qualquer outro navegador web, sugiro que pesquise na internet.\n",
    "\n",
    "# Substitua o seu-usuario e sua-pasta-com-o-chrome-driver abaixo. O caminho precisa do arquivo precisa ser completo, não pode estar abreviado.\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\seu-usuario\\sua-pasta-com-o-chrome-driver\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora iremos criar a classe que será responsável pela extração dos dados\n",
    "class ReclameAqui:\n",
    "\n",
    "    # URL base do site do Reclame Aqui\n",
    "    base_url = \"https://www.reclameaqui.com.br/empresa/\"\n",
    "    \n",
    "    # Inicializador da classe\n",
    "    def __init__(self, driver, company):\n",
    "        self.driver = driver\n",
    "        self.company = company\n",
    "\n",
    "    # Aqui iremos extrair as informações básicas das reclamações, como o título, o link da reclamação,\n",
    "    # de que cidade ela foi feita, há quanto tempo foi feita e qual o seu status na plataforma.\n",
    "    def extract_information(self, number_of_pages):\n",
    "        url = self.base_url + self.company + \"/lista-reclamacoes/?pagina=\"\n",
    "        self.complaints, self.titles, self.links = [], [], []\n",
    "\n",
    "        for i in range(1, number_of_pages + 1):\n",
    "            self.driver.get(url + str(i))\n",
    "            sleep(3)\n",
    "            html = bs(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "            complaints_html = html.find_all(\"p\", {\"class\": \"text-detail\"})\n",
    "            page_complaints = [\n",
    "                complaint.text.split(\"|\") for complaint in complaints_html\n",
    "            ]\n",
    "            self.complaints.extend(page_complaints)\n",
    "\n",
    "            titles_and_links = html.find_all(\n",
    "                \"a\", {\"class\": \"link-complain-id-complains\"}\n",
    "            )\n",
    "            self.titles.extend([title.text.strip() for title in titles_and_links])\n",
    "            self.links.extend([link.get(\"href\") for link in titles_and_links])\n",
    "\n",
    "    # Aqui iremos extrair o texto das reclamações dos clientes\n",
    "    def extract_description(self):\n",
    "        urls = [self.base_url + link[1:] for link in self.links]\n",
    "        self.description = []\n",
    "        for url in urls:\n",
    "            self.driver.get(url)\n",
    "            sleep(3)\n",
    "            html = bs(self.driver.page_source, \"html.parser\")\n",
    "            complaint = html.find(\"div\", {\"class\": \"complain-body\"}).text.strip()\n",
    "            self.description.append(complaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos a classe sempre com o browser do ChromeDriver que criamos e com o nome da empresa que queremos extrair as informações do Reclame aqui\n",
    "\n",
    "# - Exemplo: \n",
    "# Site do Reclame aqui: https://www.reclameaqui.com.br/empresa/nome-da-empresa/\n",
    "# Como ficaria a inicialização da classe:\n",
    "scrapping_reclame_aqui = ReclameAqui(browser, \"nome-da-empresa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos sempre extrair primeiro as informações porque elas nos darão o link a ser utilizado na função extrair_descricoes()\n",
    "\n",
    "# Definimos quantas páginas queremos percorrer extraindo as informações da empresa\n",
    "pages = 3\n",
    "# Extraímos as informações\n",
    "scrapping_reclame_aqui.extract_information(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora podemos coletar os textos das reclamações dos clientes\n",
    "scrapping_reclame_aqui.extract_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando os dados extraídos em arrays numpy\n",
    "titles = np.array(scrapping_reclame_aqui.titles)\n",
    "description = np.array(scrapping_reclame_aqui.description)\n",
    "complaints = np.array(scrapping_reclame_aqui.complaints)\n",
    "links = np.array(scrapping_reclame_aqui.links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
